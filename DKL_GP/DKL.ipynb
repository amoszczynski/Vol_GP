{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "daily_chains = {}\n",
    "\n",
    "for file in os.listdir('../option_data/spy_data'):\n",
    "    if file[-4:] == '.csv':\n",
    "                \n",
    "        df = pd.read_csv('../option_data/spy_data/' + file)    \n",
    "        \n",
    "        # moving to datetime and making features\n",
    "        df['quote_datetime'] = pd.to_datetime(df['quote_datetime'])\n",
    "        df['expiration'] = pd.to_datetime(df['expiration'])\n",
    "        df['quote_date'] = df['quote_datetime'][0].date()\n",
    "        df['quote_date'] = pd.to_datetime(df['quote_date'])\n",
    "        \n",
    "        # getting only 4:00 quotes\n",
    "        eod = datetime.datetime.combine(df['quote_datetime'][0].date(), datetime.time(16,0, 0))\n",
    "        df = df.loc[df['quote_datetime'] == eod]\n",
    "        \n",
    "        # getting time to expiration and moneyness\n",
    "        df['T'] = df['expiration'] - df['quote_date']\n",
    "        df['T'] = df['T'].dt.days\n",
    "        df['moneyness'] = df['active_underlying_price'] / df['strike'] \n",
    "        \n",
    "        # converting to ML features\n",
    "        df['mny'] = df['moneyness'].astype(np.float32)\n",
    "        df['iv'] = df['implied_volatility'].astype(np.float32)\n",
    "        \n",
    "        # filtering for research paper criteria\n",
    "        df = df.loc[(df['close']!=0) & \n",
    "                    (df['iv']!=0) & (df['T']>=20) & \n",
    "                    (df['T']<=365) & \n",
    "                    (df['mny']>0.7) & (df['mny']<1.3) &\n",
    "                    (df['bid_size']!=0) & df['ask_size']!=0]\n",
    "                \n",
    "        # splitting up into calls/puts\n",
    "        calls = df.loc[df['option_type']=='C'][['T', 'mny', 'bid_size', 'bid', 'ask_size', 'ask', 'iv']].astype(np.float32)\n",
    "        puts = df.loc[df['option_type']=='P'][['T', 'mny', 'bid_size', 'bid', 'ask_size', 'ask', 'iv']].astype(np.float32)\n",
    "        opts = {'calls':calls, 'puts':puts}\n",
    "    \n",
    "        # assinging to date\n",
    "        daily_chains[file[-14:-4]] = opts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = [print(k) for k in sorted(daily_chains.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HARDCODED\n",
    "data_dim = 5 #train_x.size(-1) \n",
    "\n",
    "class LargeFeatureExtractor(torch.nn.Sequential):\n",
    "    def __init__(self):\n",
    "        super(LargeFeatureExtractor, self).__init__()\n",
    "        self.add_module('linear1', torch.nn.Linear(data_dim, 100))\n",
    "        self.add_module('relu1', torch.nn.ReLU())\n",
    "        self.add_module('linear2', torch.nn.Linear(100, 20))\n",
    "        self.add_module('relu2', torch.nn.ReLU())\n",
    "        #self.add_module('linear3', torch.nn.Linear(500, 50))\n",
    "        #self.add_module('relu3', torch.nn.ReLU())\n",
    "        self.add_module('linear4', torch.nn.Linear(20, 2))\n",
    "\n",
    "feature_extractor = LargeFeatureExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "        def __init__(self, train_x, train_y, likelihood):\n",
    "            super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "            self.mean_module = gpytorch.means.ConstantMean()\n",
    "            self.covar_module = gpytorch.kernels.GridInterpolationKernel(\n",
    "                gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=2)),\n",
    "                num_dims=2, grid_size=100\n",
    "            )\n",
    "            self.feature_extractor = feature_extractor\n",
    "\n",
    "            # This module will scale the NN features so that they're nice values\n",
    "            self.scale_to_bounds = gpytorch.utils.grid.ScaleToBounds(-1., 1.)\n",
    "\n",
    "        def forward(self, x):\n",
    "            # We're first putting our data through a deep net (feature extractor)\n",
    "            projected_x = self.feature_extractor(x)\n",
    "            projected_x = self.scale_to_bounds(projected_x)  # Make the NN values \"nice\"\n",
    "\n",
    "            mean_x = self.mean_module(projected_x)\n",
    "            covar_x = self.covar_module(projected_x)\n",
    "            return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_models = {}\n",
    "#likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "\n",
    "for day, options in daily_chains.items():\n",
    "    \n",
    "    info = {}\n",
    "    \n",
    "    # calls and puts\n",
    "    c = options['calls']\n",
    "    p = options['puts']\n",
    "    \n",
    "    # feature transformations\n",
    "    c['mm_T'] = (c['T'] - 20)/(365-20)\n",
    "    c['mm_mny'] = (c['mny'] - 0.7)/(1.3-0.7)\n",
    "    c['ln_iv'] = np.log(c['iv'])\n",
    "    c['ln_bsz'] = np.log(c['bid_size'])\n",
    "    c['ln_asz'] = np.log(c['ask_size'])\n",
    "    c['ln_spread'] = np.log(c['ask'] - c['bid'])\n",
    "    \n",
    "     # feature transformations\n",
    "    p['mm_T'] = (p['T'] - 20)/(365-20)\n",
    "    p['mm_mny'] = (p['mny'] - 0.7)/(1.3-0.7)\n",
    "    p['ln_iv'] = np.log(p['iv'])\n",
    "    p['ln_bsz'] = np.log(p['bid_size'])\n",
    "    p['ln_asz'] = np.log(p['ask_size'])\n",
    "    p['ln_spread'] = np.log(p['ask'] - p['bid'])\n",
    "\n",
    "    # train test split\n",
    "    c_train, c_test = train_test_split(c, test_size=0.2)\n",
    "    p_train, p_test = train_test_split(p, test_size=0.2)\n",
    "    info['call_train'] = c_train\n",
    "    info['call_test'] = c_test\n",
    "    info['put_train'] = p_train\n",
    "    info['put_test'] = p_test\n",
    "\n",
    "    # into tensors\n",
    "    x_features = ['mm_T', 'mm_mny', 'ln_bsz', 'ln_asz', 'ln_spread']\n",
    "    cx_train = torch.tensor(c_train[x_features].values)\n",
    "    cy_train = torch.tensor(c_train[['ln_iv']].values).reshape(len(c_train))\n",
    "    cx_test = torch.tensor(c_test[x_features].values)\n",
    "    cy_test = torch.tensor(c_test[['ln_iv']].values).reshape(len(c_test))\n",
    "    \n",
    "    px_train = torch.tensor(p_train[x_features].values)\n",
    "    py_train = torch.tensor(p_train[['ln_iv']].values).reshape(len(p_train))\n",
    "    px_test = torch.tensor(p_test[x_features].values)\n",
    "    py_test = torch.tensor(p_test[['ln_iv']].values).reshape(len(p_test))\n",
    "    \n",
    "    # initializing likelihood and model\n",
    "    c_likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    c_model = GPRegressionModel(cx_train, cy_train, c_likelihood)\n",
    "    \n",
    "    p_likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    p_model = GPRegressionModel(px_train, py_train, p_likelihood)\n",
    "\n",
    "    # Find optimal model hyperparameters\n",
    "    c_model.train()\n",
    "    c_likelihood.train()\n",
    "    \n",
    "    p_model.train()\n",
    "    p_likelihood.train()\n",
    "\n",
    "    # Use the adam optimizer\n",
    "    c_optimizer = torch.optim.Adam([\n",
    "        {'params': c_model.feature_extractor.parameters()},\n",
    "        {'params': c_model.covar_module.parameters()},\n",
    "        {'params': c_model.mean_module.parameters()},\n",
    "        {'params': c_model.likelihood.parameters()},\n",
    "    ], lr=0.05)\n",
    "    p_optimizer = torch.optim.Adam([\n",
    "        {'params': p_model.feature_extractor.parameters()},\n",
    "        {'params': p_model.covar_module.parameters()},\n",
    "        {'params': p_model.mean_module.parameters()},\n",
    "        {'params': p_model.likelihood.parameters()},\n",
    "    ], lr=0.05)\n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    c_mll = gpytorch.mlls.ExactMarginalLogLikelihood(c_likelihood, c_model)\n",
    "    p_mll = gpytorch.mlls.ExactMarginalLogLikelihood(p_likelihood, p_model)\n",
    "\n",
    "    c_losses = []\n",
    "    p_losses = []\n",
    "    \n",
    "    def train_calls():\n",
    "        for i in range(500):\n",
    "            # Zero backprop gradients\n",
    "            c_optimizer.zero_grad()\n",
    "            # Get output from model\n",
    "            c_output = c_model(cx_train)\n",
    "            # Calc loss and backprop derivatives\n",
    "            c_loss = -c_mll(c_output, cy_train)\n",
    "            c_loss.backward()\n",
    "            c_optimizer.step()\n",
    "            \n",
    "            c_losses.append(c_loss.item())\n",
    "            \n",
    "    def train_puts():\n",
    "        for i in range(500):\n",
    "            # Zero backprop gradients\n",
    "            p_optimizer.zero_grad()\n",
    "            # Get output from model\n",
    "            p_output = p_model(px_train)\n",
    "            # Calc loss and backprop derivatives\n",
    "            p_loss = -p_mll(p_output, py_train)\n",
    "            p_loss.backward()\n",
    "            p_optimizer.step()\n",
    "            \n",
    "            p_losses.append(p_loss.item())\n",
    "\n",
    "    print('STARTING CALL MODEL TRAINING FOR', day)\n",
    "    train_calls()\n",
    "    \n",
    "    print('STARTING PUT MODEL TRAINING FOR', day)\n",
    "    train_puts()\n",
    "\n",
    "    info['call_losses'] = c_losses\n",
    "    info['put_losses'] = p_losses\n",
    "\n",
    "    # Get into evaluation (predictive posterior) mode\n",
    "    c_model.eval()\n",
    "    c_likelihood.eval()\n",
    "    \n",
    "    p_model.eval()\n",
    "    p_likelihood.eval()\n",
    "\n",
    "    with torch.no_grad(), gpytorch.settings.use_toeplitz(False), gpytorch.settings.fast_pred_var():\n",
    "        c_preds = c_model(cx_test)\n",
    "        p_preds = p_model(px_test)\n",
    "\n",
    "    info['call_preds'] = c_preds\n",
    "    info['put_preds'] = p_preds        \n",
    "\n",
    "    # rmse\n",
    "    c_rmse = torch.sqrt(torch.mean(torch.pow(math.e ** c_preds.mean - math.e ** cy_test, 2)))\n",
    "    p_rmse = torch.sqrt(torch.mean(torch.pow(math.e ** p_preds.mean - math.e ** py_test, 2)))\n",
    "    info['call_RMSE'] = c_rmse\n",
    "    info['put_RMSE'] = p_rmse\n",
    "    \n",
    "    # saving the model\n",
    "    torch.save(c_model, 'models/call_DKLGP_'+day+'.pt')\n",
    "    torch.save(p_model, 'models/put_DKLGP_'+day+'.pt')\n",
    "    \n",
    "    # saving likelihood\n",
    "    info['call_likelihood'] = c_likelihood\n",
    "    info['put_likelihood'] = p_likelihood\n",
    "    \n",
    "    gp_models[day] = info  \n",
    "    break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO BEAT: .0032, .0038\n",
    "print('call rmse:', c_rmse)\n",
    "print('put rmse:', p_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(p_losses)), p_losses, label='put model losses')\n",
    "plt.plot(range(len(c_losses)), c_losses, label='call model losses')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
