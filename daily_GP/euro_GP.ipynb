{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import gpytorch\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "call_exp = {\n",
    "    \"A\":\"January\",\n",
    "    \"B\":\"February\",\n",
    "    \"C\":\"March\",\n",
    "    \"D\":\"April\",\n",
    "    \"E\":\"May\",\n",
    "    \"F\":\"June\",\n",
    "    \"G\":\"July\",\n",
    "    \"H\":\"August\",\n",
    "    \"I\":\"September\",\n",
    "    \"J\":\"October\",\n",
    "    \"K\":\"November\",\n",
    "    \"L\":\"December\"\n",
    "}\n",
    "put_exp = {\n",
    "    \"M\":\"January\",\n",
    "    \"N\":\"February\",\n",
    "    \"O\":\"March\",\n",
    "    \"P\":\"April\",\n",
    "    \"Q\":\"May\",\n",
    "    \"R\":\"June\",\n",
    "    \"S\":\"July\",\n",
    "    \"T\":\"August\",\n",
    "    \"U\":\"September\",\n",
    "    \"V\":\"October\",\n",
    "    \"W\":\"November\",\n",
    "    \"X\":\"December\"\n",
    "}\n",
    "\n",
    "exp_month = {\n",
    "    \"A\":\"January\",\n",
    "    \"B\":\"February\",\n",
    "    \"C\":\"March\",\n",
    "    \"D\":\"April\",\n",
    "    \"E\":\"May\",\n",
    "    \"F\":\"June\",\n",
    "    \"G\":\"July\",\n",
    "    \"H\":\"August\",\n",
    "    \"I\":\"September\",\n",
    "    \"J\":\"October\",\n",
    "    \"K\":\"November\",\n",
    "    \"L\":\"December\",\n",
    "    \"M\":\"January\",\n",
    "    \"N\":\"February\",\n",
    "    \"O\":\"March\",\n",
    "    \"P\":\"April\",\n",
    "    \"Q\":\"May\",\n",
    "    \"R\":\"June\",\n",
    "    \"S\":\"July\",\n",
    "    \"T\":\"August\",\n",
    "    \"U\":\"September\",\n",
    "    \"V\":\"October\",\n",
    "    \"W\":\"November\",\n",
    "    \"X\":\"December\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in futures data\n",
    "import pickle \n",
    "import datetime\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "\n",
    "# open futures.pickle\n",
    "with open('futures.pickle', 'rb') as handle:\n",
    "    futures = pickle.load(handle)\n",
    "\n",
    "name_map = {'H':3, 'M':6, 'U':9, 'Z':12}\n",
    "future_chain = {}\n",
    "for es in futures:\n",
    "    date = str(name_map[es[2]]) + '-' + es[-2:]\n",
    "    future_chain[date] = futures[es]\n",
    "    future_chain[date].index = pd.to_datetime(future_chain[date].index) \n",
    "\n",
    "def build_fwd_curve(quote_date, underlying):\n",
    "    # find the futures contract that is closest to the ttm\n",
    "\n",
    "    for es in future_chain.keys():\n",
    "        if pd.to_datetime(es, format=\"%m-%y\") > quote_date:\n",
    "            break\n",
    "        \n",
    "    # find next 4 contracts and get prices on quote_date\n",
    "    dates = [quote_date]\n",
    "    for i in range(4):\n",
    "        dates.append(pd.to_datetime(es, format=\"%m-%y\") + pd.DateOffset(months=3*i))\n",
    "        \n",
    "    fwd_curve = [underlying]\n",
    "    for date in dates[1:]:\n",
    "        price = future_chain[date.strftime(\"%-m-%y\")].loc[quote_date]\n",
    "        fwd_curve.append(price.iloc[0])\n",
    "\n",
    "    \n",
    "    # interpolate to get the price at ttm\n",
    "    ttm_curve = [0]\n",
    "    for i in range(1, len(dates)):\n",
    "        ttm_curve.append((dates[i] - quote_date).days / 365)\n",
    "    \n",
    "    fwd_curve = np.array(fwd_curve)\n",
    "    ttm_curve = np.array(ttm_curve)\n",
    "\n",
    "    f = interp1d(ttm_curve, fwd_curve, kind='cubic', fill_value='extrapolate')\n",
    "\n",
    "    return f\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dumas vol surface\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def dumas_surface(df):\n",
    "    X = df['mny']\n",
    "    T = df['T']\n",
    "    \n",
    "    df['X^2'] = X**2\n",
    "    df['T^2'] = T**2\n",
    "    df['XT'] = X*T\n",
    "    \n",
    "    features = df[['mny', 'X^2', 'T', 'T^2', 'XT']]\n",
    "    target = df['iv']\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(features, target)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA PARSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_chains = {}\n",
    "for year in ['2021', '2022', '2023']:\n",
    "    \n",
    "    # load in dataset\n",
    "    df = pd.read_csv('../euro_option/elektrontimeseries_' + year + '.csv')\n",
    "\n",
    "    # necessary rows\n",
    "    df = df[['Ask', 'Bid', 'Last', 'Implied Volatility', 'RIC', 'Trade Date']]\n",
    "\n",
    "    # get underlying price\n",
    "    df_spx = df[df['RIC'] == '.SPX']\n",
    "    spx = df_spx[['Last', 'Trade Date']].copy(deep=True)\n",
    "    spx['Trade Date'] = pd.to_datetime(spx['Trade Date'])\n",
    "    spx.index = spx['Trade Date']\n",
    "    spx.drop(columns=['Trade Date'], inplace=True)\n",
    "\n",
    "    # get option data\n",
    "    df_opt = df[(df['RIC'] != '.SPX') & (df['RIC'].str.slice(8,13).str.isnumeric())]\n",
    "    opts = df_opt.copy(deep=True)\n",
    "    \n",
    "    # trade date\n",
    "    opts['quote'] = pd.to_datetime(opts['Trade Date'])\n",
    "\n",
    "    # month code\n",
    "    opts['month_code'] = opts['RIC'].str.slice(3,4)\n",
    "\n",
    "    # expiration date\n",
    "    opts['year'] = pd.to_datetime(opts['RIC'].str.slice(6,8), format='%y').dt.year\n",
    "    opts['month'] = pd.to_datetime(opts['month_code'].str.upper().map(exp_month), format='%B').dt.month\n",
    "    opts['day'] = pd.to_datetime(opts['RIC'].str.slice(4,6), format='%d').dt.day\n",
    "    opts['exp'] = pd.to_datetime(opts[['year', 'month', 'day']])\n",
    "    opts['T'] = opts['exp'] - opts['quote']\n",
    "    opts['T'] = opts['T'].dt.days\n",
    "\n",
    "    # option type\n",
    "    opts['option_type'] = np.where(opts['month_code'].str.upper().isin(call_exp.keys()), 'c', 'p')\n",
    "\n",
    "    # strike price\n",
    "    opts['k1000+'] = opts['month_code'].str.islower()\n",
    "    opts['strike'] = opts['RIC'].str.slice(8,13).astype(float)\n",
    "    opts['strike'] = np.where(opts['k1000+'], opts['strike']/10, opts['strike']/100)\n",
    "\n",
    "    # underlying\n",
    "    opts = opts.merge(spx, left_on='quote', right_on='Trade Date', how='left')\n",
    "    opts.rename(columns={'Last_x':'last', 'Last_y':'underlying'}, inplace=True)\n",
    "\n",
    "    # midprice\n",
    "    opts['midprice'] = (opts['Ask'] + opts['Bid']) / 2\n",
    "\n",
    "    # build forward curves\n",
    "    fwd_curves = {}\n",
    "    for quote_date in opts['quote'].unique():\n",
    "        fwd_curves[quote_date] = build_fwd_curve(quote_date, opts.loc[opts['quote'] == quote_date, 'underlying'].iloc[0])\n",
    "    \n",
    "    opts['fwd_price'] = opts.apply(lambda x: fwd_curves[x['quote']](x['T']/365), axis=1)\n",
    "    opts['moneyness'] = opts['fwd_price'] / opts['strike']\n",
    "    \n",
    "    # converting to ML features\n",
    "    opts['T'] = opts['T'].astype(np.float32)\n",
    "    opts['mny'] = opts['moneyness'].astype(np.float32)\n",
    "    opts['iv'] = opts['Implied Volatility'].astype(np.float32) / 100\n",
    "    \n",
    "    # filtering\n",
    "    opts = opts.loc[(~opts['iv'].isna()) & \n",
    "                    (opts['T']>=20) & (opts['T']<=365) & \n",
    "                    (opts['mny']>0.7) & (opts['mny']<1.3)]\n",
    "\n",
    "\n",
    "    # split up per day\n",
    "    for quote_date in opts['quote'].unique():\n",
    "    \n",
    "        # splitting up into calls/puts\n",
    "        calls = opts.loc[(opts['quote'] == quote_date) & (opts['option_type']=='c')][['T', 'mny', 'iv']]\n",
    "        puts = opts.loc[(opts['quote'] == quote_date) & (opts['option_type']=='p')][['T', 'mny', 'iv']]\n",
    "        \n",
    "        # ensure enough data        \n",
    "        if len(calls) < 10 or len(puts) < 10:\n",
    "            continue\n",
    "        \n",
    "        calls_puts = {'calls':calls, 'puts':puts}\n",
    "        daily_chains[f'{quote_date.date()}'] = calls_puts\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort daily_chains\n",
    "daily_chains = dict(sorted(daily_chains.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GP DEFINITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the simplest form of GP model, exact inference\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_split(df, n_bins, test_size=0.2):\n",
    "    \n",
    "    # bin moneyness\n",
    "    grouped = df.groupby(['T'])\n",
    "    \n",
    "    train_dfs = []\n",
    "    test_dfs = []\n",
    "    \n",
    "    for _, group in grouped:\n",
    "        # if not enough data\n",
    "        if len(group)//n_bins <= 1/test_size:\n",
    "            train, test = train_test_split(group, test_size=test_size)\n",
    "            \n",
    "        # split into bins\n",
    "        else:\n",
    "            group['bins'] = pd.qcut(group['mny'], n_bins, duplicates='drop', labels=False)\n",
    "            train, test = train_test_split(group, test_size=test_size, stratify=group['bins'])\n",
    "\n",
    "        train_dfs.append(train)\n",
    "        test_dfs.append(test)\n",
    "    \n",
    "    train_df = pd.concat(train_dfs)\n",
    "    test_df = pd.concat(test_dfs)\n",
    "    \n",
    "    return train_df, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"is_sparse is deprecated and will be removed in a future version\")\n",
    "\n",
    "gp_models = {}\n",
    "\n",
    "for day, options in list(daily_chains.items()):\n",
    "    \n",
    "    info = {}\n",
    "    \n",
    "    # calls and puts\n",
    "    c = options['calls']\n",
    "    p = options['puts']\n",
    "        \n",
    "    # feature transformations\n",
    "    c['mm_T'] = (c['T'] - 20)/(365-20)\n",
    "    c['mm_mny'] = (c['mny'] - 0.7)/(1.3-0.7)\n",
    "    c['ln_iv'] = np.log(c['iv'])\n",
    "\n",
    "    p['mm_T'] = (p['T'] - 20)/(365-20)\n",
    "    p['mm_mny'] = (p['mny'] - 0.7)/(1.3-0.7)\n",
    "    p['ln_iv'] = np.log(p['iv'])\n",
    "    \n",
    "    # test/train split\n",
    "    c_train, c_test = custom_split(c, 3, test_size=0.2)\n",
    "    p_train, p_test = custom_split(p, 3, test_size=0.2)\n",
    "    info['call_train'] = c_train\n",
    "    info['call_test'] = c_test\n",
    "    info['put_train'] = p_train\n",
    "    info['put_test'] = p_test\n",
    "    \n",
    "    # into tensors\n",
    "    cx_train = torch.tensor(c_train[['mm_T', 'mm_mny']].values)\n",
    "    cy_train = torch.tensor(c_train[['ln_iv']].values).reshape(len(c_train))\n",
    "    cx_test = torch.tensor(c_test[['mm_T', 'mm_mny']].values)\n",
    "    cy_test = torch.tensor(c_test[['ln_iv']].values).reshape(len(c_test))\n",
    "    \n",
    "    px_train = torch.tensor(p_train[['mm_T', 'mm_mny']].values)\n",
    "    py_train = torch.tensor(p_train[['ln_iv']].values).reshape(len(p_train))\n",
    "    px_test = torch.tensor(p_test[['mm_T', 'mm_mny']].values)\n",
    "    py_test = torch.tensor(p_test[['ln_iv']].values).reshape(len(p_test))\n",
    "    \n",
    "    #break\n",
    "    # initializing likelihood and model\n",
    "    c_likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    c_model = ExactGPModel(cx_train, cy_train, c_likelihood)  \n",
    "    \n",
    "    p_likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    p_model = ExactGPModel(px_train, py_train, p_likelihood)  \n",
    "    \n",
    "    # finding optimal model parameters\n",
    "    c_model.train()\n",
    "    c_likelihood.train()\n",
    "\n",
    "    p_model.train()\n",
    "    p_likelihood.train()\n",
    "    \n",
    "    # Use the adam optimizer\n",
    "    c_optimizer = torch.optim.Adam(c_model.parameters(), lr=0.05) \n",
    "    p_optimizer = torch.optim.Adam(p_model.parameters(), lr=0.05) \n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    c_mll = gpytorch.mlls.ExactMarginalLogLikelihood(c_likelihood, c_model)\n",
    "    p_mll = gpytorch.mlls.ExactMarginalLogLikelihood(p_likelihood, p_model)\n",
    "    c_losses = []\n",
    "    p_losses = []\n",
    "    \n",
    "    training_iter = 250\n",
    "    print('STARTING CALL GP TRAINING FOR ', day)\n",
    "    for i in range(training_iter):\n",
    "        # Zero gradients from previous iteration\n",
    "        c_optimizer.zero_grad()\n",
    "        # Output from model\n",
    "        c_output = c_model(cx_train)\n",
    "        # Calc loss and backprop gradients\n",
    "        c_loss = -c_mll(c_output, cy_train)\n",
    "        c_loss.backward()\n",
    "        \n",
    "        c_losses.append(c_loss.item())\n",
    "        if i % 5 == -1:\n",
    "            print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "                i + 1, training_iter, c_loss.item(),\n",
    "                c_model.covar_module.base_kernel.lengthscale.item(),\n",
    "                c_model.likelihood.noise.item()\n",
    "            ))\n",
    "        c_optimizer.step()\n",
    "        \n",
    "    print('STARTING PUT GP TRAINING FOR ', day)\n",
    "    for i in range(training_iter):\n",
    "        # Zero gradients from previous iteration\n",
    "        p_optimizer.zero_grad()\n",
    "        # Output from model\n",
    "        p_output = p_model(px_train)\n",
    "        # Calc loss and backprop gradients\n",
    "        p_loss = -p_mll(p_output, py_train)\n",
    "        p_loss.backward()\n",
    "        \n",
    "        p_losses.append(p_loss.item())\n",
    "        if i % 5 == -1:\n",
    "            print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "                i + 1, training_iter, p_loss.item(),\n",
    "                p_model.covar_module.base_kernel.lengthscale.item(),\n",
    "                p_model.likelihood.noise.item()\n",
    "            ))\n",
    "        p_optimizer.step()\n",
    "    \n",
    "    info['call_losses'] = c_losses\n",
    "    info['put_losses'] = p_losses\n",
    "\n",
    "    # Get into evaluation (predictive posterior) mode\n",
    "    c_model.eval()\n",
    "    c_likelihood.eval()\n",
    "    \n",
    "    p_model.eval()\n",
    "    p_likelihood.eval()\n",
    "\n",
    "    # Make predictions by feeding model through likelihood\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        c_preds = c_likelihood(c_model(cx_test))\n",
    "        p_preds = p_likelihood(p_model(px_test))\n",
    "\n",
    "    info['call_preds'] = c_preds\n",
    "    info['put_preds'] = p_preds        \n",
    "\n",
    "    # rmse\n",
    "    c_rmse = torch.sqrt(torch.mean(torch.pow(math.e ** c_preds.mean - math.e ** cy_test, 2)))\n",
    "    p_rmse = torch.sqrt(torch.mean(torch.pow(math.e ** p_preds.mean - math.e ** py_test, 2)))\n",
    "    info['call_RMSE'] = c_rmse\n",
    "    info['put_RMSE'] = p_rmse\n",
    "    \n",
    "    # saving the model\n",
    "    torch.save(c_model, 'e_models/call_GP_'+day+'.pt')\n",
    "    torch.save(p_model, 'e_models/put_GP_'+day+'.pt')\n",
    "    \n",
    "    # saving likelihood\n",
    "    info['call_likelihood'] = c_likelihood\n",
    "    info['put_likelihood'] = p_likelihood\n",
    "    \n",
    "    gp_models[day] = info  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "for day in daily_chains.keys():\n",
    "    c = daily_chains[day]['calls']\n",
    "    p = daily_chains[day]['puts']\n",
    "    try:\n",
    "        custom_split(c, 3, 0.2)\n",
    "        custom_split(p, 3, 0.2)\n",
    "    except:\n",
    "        print(day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLOTTING LOSSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding time ot expirations for a specific day\n",
    "day_str = '2023-10-02'\n",
    "print('valid times to exp:', sorted(daily_chains[day_str]['calls']['T'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting losses\n",
    "c_plt_loss = gp_models[day_str]['call_losses']\n",
    "p_plt_loss = gp_models[day_str]['put_losses']\n",
    "\n",
    "plt.plot(range(len(c_plt_loss)), c_plt_loss, label='call loss')\n",
    "plt.plot(range(len(p_plt_loss)), p_plt_loss, label='put loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLOTTING VOL SMILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_to_exp = 354\n",
    "\n",
    "# getting training/test data\n",
    "c_tr = gp_models[day_str]['call_train']\n",
    "c_tr = c_tr.loc[c_tr['T']==t_to_exp]\n",
    "c_tst = gp_models[day_str]['call_test']\n",
    "c_tst = c_tst.loc[c_tst['T']==t_to_exp]\n",
    "\n",
    "p_tr = gp_models[day_str]['put_train']\n",
    "p_tr = p_tr.loc[p_tr['T']==t_to_exp]\n",
    "p_tst = gp_models[day_str]['put_test']\n",
    "p_tst = p_tst.loc[p_tst['T']==t_to_exp]\n",
    "\n",
    "# getting model/likelihood\n",
    "c_plt_model = torch.load('e_models/call_GP_'+day_str+'.pt')\n",
    "c_plt_model.eval()\n",
    "p_plt_model = torch.load('e_models/put_GP_'+day_str+'.pt')\n",
    "p_plt_model.eval()\n",
    "c_ll = gp_models[day_str]['call_likelihood']\n",
    "c_ll.eval()\n",
    "p_ll = gp_models[day_str]['put_likelihood']\n",
    "p_ll.eval()\n",
    "\n",
    "# sample data\n",
    "rng_mny = np.array(np.linspace(0.7, 1.3, 100)).astype(np.float32)\n",
    "samp_mny = (rng_mny - 0.7) / (1.3 - 0.7)\n",
    "t_arr = np.array([t_to_exp]*100).astype(np.float32)\n",
    "samp_t = (t_arr - 20) / (365 - 20)\n",
    "\n",
    "sample = torch.tensor(np.array([samp_t, samp_mny])).T #.reshape((1000, 2))\n",
    "\n",
    "# Make predictions by feeding model through likelihood\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    c_plt = c_ll(c_plt_model(sample))\n",
    "    p_plt = p_ll(p_plt_model(sample))\n",
    "    \n",
    "# make dumas vol surface as well\n",
    "c_dumas_model = dumas_surface(c_tr.copy(deep=True))\n",
    "p_dumas_model = dumas_surface(p_tr.copy(deep=True))\n",
    "\n",
    "c_dumas_preds = c_dumas_model.predict(np.array([rng_mny, rng_mny**2, t_arr, t_arr**2, rng_mny*t_arr]).T)\n",
    "p_dumas_preds = p_dumas_model.predict(np.array([rng_mny, rng_mny**2, t_arr, t_arr**2, rng_mny*t_arr]).T)\n",
    "\n",
    "c_dumas = pd.DataFrame({'mny':rng_mny, 'T':t_arr, 'iv':c_dumas_preds})\n",
    "p_dumas = pd.DataFrame({'mny':rng_mny, 'T':t_arr, 'iv':p_dumas_preds})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calls\n",
    "with torch.no_grad():\n",
    "    # Initialize plot\n",
    "    f, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "    # Get upper and lower confidence bounds\n",
    "    lower, upper = c_plt.confidence_region()\n",
    "    \n",
    "    # Plot training data as black stars\n",
    "    ax.plot(c_tr['mny'], c_tr['iv'], 'k*', label='training points')\n",
    "    \n",
    "    # Plot testing data as blue dots\n",
    "    ax.plot(c_tst['mny'], c_tst['iv'], 'b.', label='testing points')\n",
    "    \n",
    "    # undo exponentiation\n",
    "    lower = math.e ** lower.numpy() #np.log(lower.numpy())\n",
    "    upper = math.e ** upper.numpy() #np.log(upper.numpy() ) \n",
    "    preds = math.e ** c_plt.mean.numpy() #np.log(c_plt.mean.numpy())\n",
    "    \n",
    "    # undo x-axis scaling by using rng_mny\n",
    "    ax.plot(rng_mny, preds, 'r', label='mean function')\n",
    "    \n",
    "    # plot dumas\n",
    "    #ax.plot(rng_mny, c_dumas_preds, 'g', label='Dumas IV')\n",
    "    \n",
    "    # Shade between the lower and upper confidence bounds\n",
    "    ax.fill_between(rng_mny, lower, upper, alpha=0.5, label='95% confidence')\n",
    "    \n",
    "    ax.set_xlim([0.7, 1.3])\n",
    "    ax.set_xlabel(\"Moneyness\")\n",
    "    ax.set_ylabel(\"Implied Volatility\")\n",
    "    ax.set_title(f\"Call IV Surface for a Time to Expiration of {t_to_exp} days\")\n",
    "    ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# puts\n",
    "with torch.no_grad():\n",
    "    # Initialize plot\n",
    "    f, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "    # Get upper and lower confidence bounds\n",
    "    lower, upper = p_plt.confidence_region()\n",
    "    \n",
    "    # Plot training data as black stars\n",
    "    ax.plot(p_tr['mny'], p_tr['iv'], 'k*', label='training points')\n",
    "    \n",
    "    # plot test data as blue stars\n",
    "    ax.plot(p_tst['mny'], p_tst['iv'], 'b.', label='testing points')\n",
    "    \n",
    "    # undo exponentiation\n",
    "    lower = math.e ** lower.numpy() #np.log(lower.numpy())\n",
    "    upper = math.e ** upper.numpy() #np.log(upper.numpy() ) \n",
    "    preds = math.e ** p_plt.mean.numpy() #np.log(p_plt.mean.numpy())\n",
    "    \n",
    "    # undo x-axis scaling by using original range\n",
    "    ax.plot(rng_mny, preds, 'r', label='mean function')\n",
    "    #ax.plot(rng_mny, p_dumas_preds, 'g', label='Dumas IV')\n",
    "\n",
    "    \n",
    "    # Shade between the lower and upper confidence bounds\n",
    "    ax.fill_between(rng_mny, lower, upper, alpha=0.5, label='95% confidence')\n",
    "    \n",
    "    ax.set_xlim([0.7, 1.3])\n",
    "    #ax.set_ylim([)\n",
    "    ax.set_xlabel(\"Moneyness\")\n",
    "    ax.set_ylabel(\"Implied Volatility\")\n",
    "    ax.set_title(f\"Put IV Surface for a Time to Expiration of {t_to_exp} days\")\n",
    "\n",
    "    ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'call rmse for {day_str}: {gp_models[day_str][\"call_RMSE\"]}')\n",
    "print(f'put rmse for {day_str}: {gp_models[day_str][\"put_RMSE\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_rmse_arr = []\n",
    "p_rmse_arr = []\n",
    "for day in list(daily_chains.keys()):\n",
    "    c_rmse_arr.append(gp_models[day][\"call_RMSE\"])\n",
    "    p_rmse_arr.append(gp_models[day][\"put_RMSE\"])\n",
    "    \n",
    "print(f'mean call rmse: {np.array(c_rmse_arr).mean()}')\n",
    "print(f'max call rmse: {np.array(c_rmse_arr).max()}')\n",
    "print(f'min call rmse: {np.array(c_rmse_arr).min()}')\n",
    "print()\n",
    "print(f'mean put rmse: {np.array(p_rmse_arr).mean()}')\n",
    "print(f'max put rmse: {np.array(p_rmse_arr).max()}')\n",
    "print(f'min put rmse: {np.array(p_rmse_arr).min()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_surface = [] # coord (T, mny, iv)\n",
    "p_surface = []\n",
    "\n",
    "valid_t = daily_chains[day_str]['calls']['T'].unique()\n",
    "\n",
    "# getting model/likelihood\n",
    "c_plt_model = torch.load('e_models/call_GP_'+day_str+'.pt')\n",
    "c_plt_model.eval()\n",
    "p_plt_model = torch.load('e_models/put_GP_'+day_str+'.pt')\n",
    "p_plt_model.eval()\n",
    "c_ll = gp_models[day_str]['call_likelihood']\n",
    "c_ll.eval()\n",
    "p_ll = gp_models[day_str]['put_likelihood']\n",
    "p_ll.eval()\n",
    "\n",
    "rng_mny = np.array(np.linspace(0.7, 1.3, 100)).astype(np.float32)\n",
    "samp_mny = (rng_mny - 0.7) / (1.3 - 0.7)\n",
    "\n",
    "for t_to_exp in valid_t:\n",
    "\n",
    "    # getting training/test data\n",
    "    c_tr = gp_models[day_str]['call_train']\n",
    "    c_tr = c_tr.loc[c_tr['T']==t_to_exp]\n",
    "    c_tst = gp_models[day_str]['call_test']\n",
    "    c_tst = c_tst.loc[c_tst['T']==t_to_exp]\n",
    "\n",
    "    p_tr = gp_models[day_str]['put_train']\n",
    "    p_tr = p_tr.loc[p_tr['T']==t_to_exp]\n",
    "    p_tst = gp_models[day_str]['put_test']\n",
    "    p_tst = p_tst.loc[p_tst['T']==t_to_exp]\n",
    "\n",
    "    # sample data\n",
    "    t_arr = np.array([t_to_exp]*100).astype(np.float32)\n",
    "    samp_t = (t_arr - 20) / (365 - 20)\n",
    "\n",
    "    sample = torch.tensor([samp_t, samp_mny]).T #.reshape((1000, 2))\n",
    "    # Make predictions by feeding model through likelihood\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        c_plt = c_ll(c_plt_model(sample))\n",
    "        p_plt = p_ll(p_plt_model(sample))\n",
    "    \n",
    "    # iv\n",
    "    c_func = math.e ** c_plt.mean.numpy()\n",
    "    p_func = math.e ** p_plt.mean.numpy()\n",
    "    \n",
    "    for i in range(100):\n",
    "        c_surface.append([rng_mny[i], t_arr[i], c_func[i]])\n",
    "        p_surface.append([rng_mny[i], t_arr[i], p_func[i]])\n",
    "        \n",
    "c_surface = np.array(c_surface)\n",
    "p_surface = np.array(p_surface)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "Xs = c_surface[:,0]\n",
    "Ys = c_surface[:,1]\n",
    "Zs = c_surface[:,2]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "surf = ax.plot_trisurf(Xs, Ys, Zs, cmap=cm.jet, linewidth=0)\n",
    "fig.colorbar(surf, label='Implied Volatility')\n",
    "\n",
    "ax.set_xlabel('moneyness')\n",
    "ax.set_ylabel('time to expiration')\n",
    "\n",
    "ax.xaxis.set_major_locator(MaxNLocator(5))\n",
    "ax.yaxis.set_major_locator(MaxNLocator(6))\n",
    "ax.zaxis.set_major_locator(MaxNLocator(5))\n",
    "ax.set_title('Call Volatility Surface')\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show() # or:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "Xs = p_surface[:,0]\n",
    "Ys = p_surface[:,1]\n",
    "Zs = p_surface[:,2]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "surf = ax.plot_trisurf(Xs, Ys, Zs, cmap=cm.jet, linewidth=0)\n",
    "fig.colorbar(surf, label='Implied Volatility')\n",
    "\n",
    "ax.set_xlabel('moneyness')\n",
    "ax.set_ylabel('time to expiration')\n",
    "\n",
    "ax.xaxis.set_major_locator(MaxNLocator(5))\n",
    "ax.yaxis.set_major_locator(MaxNLocator(6))\n",
    "ax.zaxis.set_major_locator(MaxNLocator(5))\n",
    "ax.set_title('Put Volatility Surface')\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show() # or:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DUMAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_dumas_rmse = []\n",
    "p_dumas_rmse = []\n",
    "\n",
    "for day_str in list(daily_chains.keys()):\n",
    "    \n",
    "    c_tr = gp_models[day_str]['call_train']\n",
    "    p_tr = gp_models[day_str]['put_train']\n",
    "    c_tst = gp_models[day_str]['call_test']\n",
    "    p_tst = gp_models[day_str]['put_test']\n",
    "        \n",
    "    # make dumas vol surface as well\n",
    "    c_dumas_model = dumas_surface(c_tr.copy(deep=True))\n",
    "    p_dumas_model = dumas_surface(p_tr.copy(deep=True))\n",
    "    \n",
    "    cmny = c_tst['mny'].values\n",
    "    c_t = c_tst['T'].values\n",
    "    \n",
    "    pmny = p_tst['mny'].values\n",
    "    p_t = p_tst['T'].values\n",
    "    \n",
    "    c_dumas_preds = c_dumas_model.predict(np.array([cmny, cmny**2, c_t, c_t**2, cmny*c_t]).T)\n",
    "    p_dumas_preds = p_dumas_model.predict(np.array([pmny, pmny**2, p_t, p_t**2, pmny*p_t]).T)\n",
    "    \n",
    "    c_rmse = np.sqrt(np.mean((c_dumas_preds - c_tst['iv'].values)**2))\n",
    "    p_rmse = np.sqrt(np.mean((p_dumas_preds - p_tst['iv'].values)**2))\n",
    "\n",
    "    c_dumas_rmse.append(c_rmse)\n",
    "    p_dumas_rmse.append(p_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_rmse_arr = []\n",
    "p_rmse_arr = []\n",
    "for day in list(daily_chains.keys()):\n",
    "    c_rmse_arr.append(gp_models[day][\"call_RMSE\"])\n",
    "    p_rmse_arr.append(gp_models[day][\"put_RMSE\"])\n",
    "\n",
    "print('dumas:', np.mean(c_dumas_rmse), np.mean(p_dumas_rmse))\n",
    "print('gp:', np.mean(c_rmse_arr), np.mean(p_rmse_arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRICING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as ss\n",
    "\n",
    "def bs(S0, K, T, r, sigma, payoff):\n",
    "    d1 = (np.log(S0 / K) + (r + sigma**2 / 2) * T) / (sigma * np.sqrt(T))\n",
    "    d2 = (np.log(S0 / K) + (r - sigma**2 / 2) * T) / (sigma * np.sqrt(T))\n",
    "\n",
    "    if payoff==\"call\":\n",
    "        return S0 * ss.norm.cdf(d1) - K * np.exp(-r * T) * ss.norm.cdf(d2)\n",
    "    elif payoff==\"call\":\n",
    "        return K * np.exp(-r * T) * ss.norm.cdf(-d2) - S0 * ss.norm.cdf(-d1)\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "\n",
    "def get_rfr(mp, S0, K, T, sigma, payoff):\n",
    "    \n",
    "    # binary search between 0 and 1\n",
    "    min_rfr = 0\n",
    "    max_rfr = 1\n",
    "    rfr = 0.5\n",
    "    while max_rfr - min_rfr > 0.001:\n",
    "        bsp = bs(S0, K, T, rfr, sigma, payoff)\n",
    "        if bsp < mp:\n",
    "            max_rfr = rfr\n",
    "            rfr = (rfr + min_rfr) / 2\n",
    "        else:\n",
    "            min_rfr = rfr\n",
    "            rfr = (rfr + max_rfr) / 2\n",
    "    \n",
    "    return rfr\n",
    "\n",
    "def model_price(mp, S0, K, T, sigma, pred, payoff):\n",
    "    \n",
    "    rfr = get_rfr(mp, S0, K, T, sigma, payoff)\n",
    "    return bs(S0, K, T, rfr, pred, payoff)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Derive risk-free rate and then price with predicted IV\n",
    "\n",
    "for day_str in list(daily_chains.keys())[:1]:\n",
    "\n",
    "    c_tst = gp_models[day_str]['call_test']\n",
    "    c = daily_chains[day_str]['calls']\n",
    "    p_tst = gp_models[day_str]['put_test']\n",
    "    p = daily_chains[day_str]['puts']\n",
    "    \n",
    "    c_tst['midprice'] = c.loc[c_tst.index, 'midprice']\n",
    "    c_tst['underlying'] = c.loc[c_tst.index, 'underlying']\n",
    "    p_tst['midprice'] = p.loc[p_tst.index, 'midprice']\n",
    "    p_tst['underlying'] = p.loc[p_tst.index, 'underlying']\n",
    "    c_tst['preds'] = gp_models[day_str]['call_preds'].mean.numpy()\n",
    "    p_tst['preds'] = gp_models[day_str]['put_preds'].mean.numpy()\n",
    "    \n",
    "    \n",
    "    cprices = c_tst.apply(lambda x: model_price(x['midprice'], x['underlying'], x['strike'], x['T'], x['iv'], x['pred'], 'call'), axis=1)\n",
    "    pprices = p_tst.apply(lambda x: model_price(x['midprice'], x['underlying'], x['strike'], x['T'], x['iv'], x['pred'], 'put'), axis=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save daily_chain to pickles/chain_info.pickle \n",
    "with open('pickles/chain_info.pickle', 'wb') as handle:\n",
    "    pickle.dump(daily_chains, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('pickles/model_info.pickle', 'wb') as handle:\n",
    "    pickle.dump(gp_models, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "daily_chains['2022-02-04']['calls']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vol3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
